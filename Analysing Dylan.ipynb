{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the things\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "\n",
    "# Apply Gensim topic modelling to dylan lyrics:\n",
    "from gensim import models, corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_chords = [\"i'm\",\"like\", \"well\",\"well,\", \"got\", \"\", \"know\", \"ain't\", \"get\", \"em\",\"oh,\", \"s\", \"t\", \"d\", \"ll\"]\n",
    "stop_words = stopwords.words(\"english\")   \n",
    "stop_words.extend(stop_chords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    song = BeautifulSoup(open(filename), 'html.parser')\n",
    "    song_info = get_song_information(song)\n",
    "    return song_info\n",
    "\n",
    "def get_song_information(song):\n",
    "    song_info = {}\n",
    "    song_name = song.title.text\n",
    "    album_name = song.find('a', {'class' : 'recordlink'}).text.strip()\n",
    "    lyrics = song.find('pre', {'class' : 'verse'}).text.strip()\n",
    "    lyrics = clean_up_lyrics(lyrics)\n",
    "    song_info['song'] = song_name\n",
    "    song_info['album'] = album_name\n",
    "    song_info['lyrics'] = lyrics\n",
    "    \n",
    "    return song_info\n",
    "\n",
    "def clean_up_lyrics(lyrics):\n",
    "    lyrics = lyrics.replace('\\n', \" \") # Remove newline character\n",
    "    lyrics = re.sub(\"\\s+\", \" \", lyrics) # Remove all whitespace between the lyrics\n",
    "    lyrics = toLower(lyrics)\n",
    "    #lyrics = removeStopWords(lyrics)\n",
    "    return lyrics\n",
    "\n",
    "def toLower(lyrics):\n",
    "    words = []\n",
    "    for w in lyrics:\n",
    "        words.append(w.lower())\n",
    "    return \"\".join(words)\n",
    "\n",
    "def removeStopWords(words):\n",
    "    words = words.split(\" \")\n",
    "    \n",
    "    ns_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            ns_words.append(word)\n",
    "    return \" \".join(ns_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_song_paths():\n",
    "    all_files = []\n",
    "    for(dirpath, dirnames, filenames) in os.walk('./chords/'):\n",
    "        for filename in filenames:\n",
    "            if(filename.endswith('.htm') & (not \"index\" in filename)):\n",
    "                all_files.append(os.sep.join([dirpath, filename]))\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = get_all_song_paths()\n",
    "all_song_info = []\n",
    "\n",
    "for file in all_files:\n",
    "    song = read_file(file)\n",
    "    all_song_info.append(song) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_song_info, columns=[\"song\", \"album\", \"lyrics\"])\n",
    "\n",
    "df.to_pickle('../quora_sentiment/dylan/dylan_corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['tokens'] = df['lyrics'].apply(tokenizer.tokenize)\n",
    "\n",
    "dylan_eras = {\n",
    "    '1': [\"Bob Dylan\",\"Freewheelin'\",\"The Times They Are A-changin'\", \"Another Side Of Bob Dylan\"],\n",
    "    '2': [\"Highway 61 Revisited\",\"Bringing It All Back Home\",\"Blonde on Blonde\"],\n",
    "    '3': [\"Nashville Skyline\",\"Self Portrait\",\"New Morning\",\"Pat Garret & Billy The Kid\",\"John Wesley Harding\"],\n",
    "    '4': [\"Planet Waves\",\"Dylan\",\"Blood on the Tracks\",\"Desire\"]\n",
    "}\n",
    "\n",
    "def get_dylan_era(album):\n",
    "    for k,v in dylan_eras.items():\n",
    "        if(album in v):\n",
    "            return k\n",
    "\n",
    "df['era'] = df['album'].apply(lambda x: get_dylan_era(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_words = ['i','s','ll','a','ve','d','t'] #weird tokens.\n",
    "all_words = [word for tokens in df['tokens'] for word in tokens if word not in non_words]\n",
    "sentence_lengths = [len(tokens) for tokens in df['tokens']]\n",
    "VOCAB = sorted(list(set(all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_all_words = Counter(all_words)\n",
    "count_all_words.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 10\n",
    "data = []\n",
    "for l in all_song_info:\n",
    "    data.append(l['lyrics'])\n",
    "\n",
    "def clean_text(text):\n",
    "    tokenised_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [t for t in tokenised_text if t not in stop_words and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_data = []\n",
    "for text in data:\n",
    "    tokenised_data.append(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tokenised_data)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenised_data]\n",
    "\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    "\n",
    "lsi_model = models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(NUM_TOPICS):\n",
    "    print(\"Topic #%s\" % idx, lsi_model.print_topic(idx, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can perform similarity queries with gensim\n",
    "from gensim import similarities\n",
    "# Get unseen lyric to match the topic:\n",
    "sent = \"\"\"\n",
    "Now, boys, don't start to ramble round,\n",
    "On this road of sin or you're sorrow bound.\n",
    "And you'll get lost, you'll curse the day\n",
    "You started rollin' down that lost highway.\n",
    "\n",
    "I'm a rolling stone, all alone and lost,\n",
    "For a life of sin, I have paid the cost.\n",
    "When I pass by, you'll curse the day\n",
    "You started rollin' down that lost highway.\n",
    "\"\"\"\n",
    "\n",
    "bow = dictionary.doc2bow(clean_text(sent))\n",
    "\n",
    "lda_index = similarities.MatrixSimilarity(lda_model[corpus])\n",
    "\n",
    "similarities = lda_index[lda_model[bow]]\n",
    "\n",
    "similarities = sorted(enumerate(similarities), key= lambda item: -item[1])\n",
    "\n",
    "document_id, similarity = similarities[0]\n",
    "\n",
    "print(data[document_id][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "res = vectorizer.fit_transform(df['lyrics'])\n",
    "idf = vectorizer.idf_\n",
    "print(dict(zip(vectorizer.get_feature_names(), idf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
